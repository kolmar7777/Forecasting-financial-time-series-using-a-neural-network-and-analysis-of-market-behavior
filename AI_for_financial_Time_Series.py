# -*- coding: utf-8 -*-
"""Course_work_3_ver_1_0_0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XFbjhH8B67ExEmots8k1tadC8xlF2Jm8
"""

#!pip install simfin
import sys
import simfin as sf
from simfin.names import *
import numpy as np
import pandas as pd
import private_data
import data
import tests


# Set your API-key for downloading data.
# Replace YOUR_API_KEY with your actual API-key.
#sf.set_api_key('tNldRNETtS6zd3u04i5V1rk8vSu6uYlH')

print ('Number of arguments:', len(sys.argv), 'arguments.')
print ('Argument List:', str(sys.argv))
if (len(sys.argv) == 1):
    is_data_from_file = False
elif (len(sys.argv) == 2):
    is_data_from_file = True
    filename_load = str(sys.argv[1])
else: 
    print("Usage:\npython3 python_filename.py (filename_to_dataload.txt)\n")
    exit()

sf.set_api_key(private_data.My_private_data().API_key)
assert private_data.My_private_data().API_key != 'Your_Access_Key', 'Set your API-key for downloading data. Replace  API_key with your actual API-key in private_data.py\n.'
# Set the local directory where data-files are stored.
# The dir will be created if it does not already exist.
sf.set_data_dir('~/simfin_data/')

"""
*Для дальнейших разработок с другими финансовыми показателями*"""

#comp_balance = balance[df_balance['Ticker'] == 'MSFT'].drop(columns=['Ticker', 'SimFinId'])
#comp_balance = comp_balance.rename(columns={'Report Date' : 'Date'})
#comp_price = prices[prices['Ticker'] == 'MSFT'].drop(columns=['Ticker', 'SimFinId'])

#df_companies = sf.load_companies(index= 'IndustryId', market='us')

#df_industries = sf.load_industries()

"""**STARTING OUR WORK**

Загружаем данные
"""

# Download the data from the SimFin server and load into a Pandas DataFrame.
df = sf.load_shareprices(variant='daily', market='us')
My_data = data.Data(df = df, key_Ticker_to_predict = "AAL", is_data_from_file = is_data_from_file)#, Our_First_date_which_we_wanna_start = )
 
#print(My_data.Time_series_space[0:10])

tester = tests.test_data(df = df, My_data = My_data)
tester.test_correctness_df()

"""Срезаем последний столбец матрицы (это необходимо в виду рекурентного определения тренда и проблемы с граничными точками)"""
r'''
Time_series_space_end = []
for row in Time_series_space:
  Time_series_space_end.append(row[:-1])
print(Time_series_space_end[1][:])

"""Проверка на корректность размерностей"""

current_len_of_row = len(Time_series_space_end[0])
#print(len(Time_series_space[0]))
for row in Time_series_space_end:
  if (len(row) != current_len_of_row): # Эта цифра всё время меняется! Узнаём актуальную через len(Time_series_space[0])
    print(len(row))
if (len(Key_ticker_values_list) != current_len_of_row):
  print(len(Key_ticker_values_list), current_len_of_row)

"""Перевод в NumPy array"""

len_of_each_row = len(Time_series_space_end[0])
len_of_each_column = len(Time_series_space_end)
temp = []
for row in Time_series_space_end:
  for element in row:
    temp.append(element)
matr = np.array(temp).reshape(len_of_each_column, len_of_each_row)
print(matr)

"""***Задаём границу для определения тренда!***"""

Boundary_for_neutral_trend = 1 # 1% DR

"""формируем вектор меток, соответствующий классам тренда."""

#выделяем ключевой тикер как вектор меток: 1 - нет роста на утро следующего дня для ключевого тикера(падение цены). 2 - есть, 0 - падение.
Key_ticker_values_dict = df.loc[key_Ticker_to_predict].to_dict()['Open']
Key_ticker_values_list1 = []
dates_of_Key_ticker = list(Key_ticker_values_dict)
if (Check_for_equality_of_two_times_lists(dates_of_Key_ticker, dates_for_prev_ticker) == 0):
  for date in dates_of_Key_ticker[1:]:
    if date == dates_of_Key_ticker[1]:
      prev_date = date
      continue
    if abs(((Key_ticker_values_dict[date] - Key_ticker_values_dict[prev_date]) / Key_ticker_values_dict[prev_date])*100) <= Boundary_for_neutral_trend:
      Key_ticker_values_list1.append([0,1,0].index(1))
    elif ((Key_ticker_values_dict[date] - Key_ticker_values_dict[prev_date]) / Key_ticker_values_dict[prev_date])*100 > Boundary_for_neutral_trend: # Изменение больше 1 % вверх
      Key_ticker_values_list1.append([0,0,1].index(1)) # Восходящий
    else:
      Key_ticker_values_list1.append([1,0,0].index(1)) # Нисходящий
    
    prev_date = date
print(len(Key_ticker_values_list1), len(matr[0]), Key_ticker_values_list1)

"""Формируем матрицу со значениями тренда"""

trend_matr = []
for i in range(len(matr)):
  trend_row = []
  for j in range(len(matr[i])):
    if j == 0:
      prev_value = matr[i][j]
      value = matr[i][j]
      trend_row.append(int(1))
      continue
    value = matr[i][j]
    if abs(((value - prev_value) / prev_value)*100) <= Boundary_for_neutral_trend:
      trend_row.append(int(1))
    elif ((value - prev_value) / prev_value)*100 > Boundary_for_neutral_trend: # Изменение больше 1 % вверх
      trend_row.append(int(2)) # Восходящий
    else:
      trend_row.append(int(0)) # Нисходящий
    prev_value = value
  trend_matr.append(trend_row)
  #print(trend_row)

print("Примеров = ", len(trend_matr[0]), ",Признаков = " , len(trend_matr))

"""Нормируем матрицу, которая была получена для значений цены"""

temp_1 = []
for row in matr:
  new_row = row / np.linalg.norm(row)
  temp_1.append(new_row)

normed_matr = np.array(temp_1).reshape(len_of_each_column, len_of_each_row)
print(type(normed_matr))
print(normed_matr)

"""Очередная проверка размерностей"""

current_len_of_row = len(trend_matr[0])
#print(len(Time_series_space[0]))
for row in trend_matr:
  if (len(row) != current_len_of_row): # Эта цифра всё время меняется! Узнать актуальную через len(Time_series_space[0])
    print(len(row))
if (len(Key_ticker_values_list1) != current_len_of_row):
  print(len(Key_ticker_values_list1))

"""Подключаем Torch и переводим матрицу в тензор"""

import torch
data = torch.from_numpy(normed_matr)

#Check_for_key_ticker_correctness
if(len(trend_matr[0]) == len(Key_ticker_values_list1)):
  print("CORRECT!")
print(len(trend_matr[0]))
print(len(Time_series_space_end[0]))
print(len(Key_ticker_values_list1))

print(len(trend_matr))

Dataset1 = np.array(trend_matr).T
len(Dataset1)

#Отрезаем начало, где тренд везде нейтральный.
print(Dataset1[0:2], Key_ticker_values_list1[0:2])
Key_ticker_values_list = Key_ticker_values_list1[1:]
Dataset = Dataset1[1:]
print(len(Dataset), len(Key_ticker_values_list))
print(Dataset[0], Key_ticker_values_list[0])

"""Эта утилита нужна исключительно для отображения информации об обучении и упрощения интерфейса в самом процессе обучения."""

if 'google.colab' in str(get_ipython()):
    import ssl
    import sys

    #!wget https://raw.githubusercontent.com/adasegroup/ML2020_seminars/master/seminar13/utils.py -P local_modules -nc
    sys.path.append('local_modules')

    ssl._create_default_https_context = ssl._create_unverified_context

from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
import torch
import torchvision
from tqdm import trange

import utils as seminar_utils

#loss_function = torch.nn.MSELoss()
loss_function = torch.nn.CrossEntropyLoss()

"""Разбиваем выборку на обучающую и валидационную"""

length_of_Dataset = len(Dataset)
Key_ticker_values_list_test = Key_ticker_values_list
train_data = torch.as_tensor(Dataset[ : int(length_of_Dataset*3/4)]).type(torch.float32)
train_target = torch.as_tensor(Key_ticker_values_list[ : int(length_of_Dataset*3/4)]).type(torch.float32)
test_data = torch.as_tensor(Dataset[int(length_of_Dataset*3/4) : ]).type(torch.float32)
test_target = torch.as_tensor(Key_ticker_values_list_test[int(length_of_Dataset*3/4) : ]).type(torch.float32)
#train_data = torch.as_tensor(Dataset[ : int(length_of_Dataset*3/4)]).type(torch.float32)
#train_target = torch.as_tensor(Key_ticker_values_list[ : int(length_of_Dataset*3/4)]).type(torch.int)
#test_data = torch.as_tensor(Dataset[int(length_of_Dataset*3/4) : ]).type(torch.float32)
#test_target = torch.as_tensor(Key_ticker_values_list_test[int(length_of_Dataset*3/4) : ]).type(torch.int)
#Testing
print(len(train_data) == len(train_target))
print(len(test_data) == len(test_target))
print("Длина тестовой выборки = ", len(test_data))
print("Длина обучающей выборки = ", len(train_data))
print(len(test_data)/ len(train_data) * 100,"%")

"""Посмотрим когда примерно начинается наша тестовая валидационная выборка."""

Key_ticker_values_dict_bla_bla = list(df.loc[key_Ticker_to_predict].to_dict()['Open'])[int(length_of_Dataset*3/4) : ]
print(Key_ticker_values_dict_bla_bla)

"""Создаём свой класс Датасет, наследуемый из соответствующего класса в модуле Torch-а"""

from torch.utils.data import Dataset

class CreateDataset(Dataset):
    def __init__(self, gen_data, targets):
        self.samples = []
        if (len(gen_data) == len(targets)):      
          for i in range(len(gen_data)):
            self.samples.append((gen_data[i], int(targets[i])))

        

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

"""Тестируем DataLoader с нашим Dataset-ом"""

train_dataset = CreateDataset(train_data, train_target)
val_dataset = CreateDataset(test_data, test_target)
print(len(train_dataset), train_dataset[0])
print(len(val_dataset), val_dataset[0])

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, num_workers=2, shuffle=False)

for batch_i, (input_tensor, target_tensor) in enumerate(train_loader):
    print(batch_i)
    print(input_tensor.shape, target_tensor.shape)
    print(target_tensor[:5])
    #break

"""Двухслойная линейная модель"""

class MLP(torch.nn.Module):
    def __init__(self, input_size, hidden_dim_size, number_of_targets):
        r"""
        Parameters
        ----------
        input_size : int
        hidden_dim_size : int
        number_of_targets : int
        """
        super().__init__()
        
        #input_size = input_image_size[0] * input_image_size[1] * input_image_size[2]
        #number_of_targets = 1
        self.mlp = torch.nn.Sequential(
            torch.nn.Linear(input_size, hidden_dim_size),
            torch.nn.ReLU(),
            torch.nn.BatchNorm1d(hidden_dim_size),
            torch.nn.Linear(hidden_dim_size, number_of_targets)
        )
        
    def forward(self, x):
        r"""
        Parameters
        ----------
        x : torch.Tensor
            of shape [batch_size, vector_size]
        
        Returns
        -------
        y : torch.Tensor
            of shape [batch_size, 1]
        """
        batch_size = len(x)
        x = x.reshape(batch_size, -1)
        return self.mlp(x)

"""Пробуем работать с батчами"""

inputs, targets = next(iter(train_loader))
inputs.shape

model = MLP(input_size = inputs.shape[1], hidden_dim_size = 120, number_of_targets = 3)
nn_outputs = model(inputs)
nn_outputs.shape
print(nn_outputs)

"""Определяем универсальную функцию тренировки нейронной сети"""

# we choose the device that we will work on --- GPU or CPU
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')


def train(model, optimizer, loss_function, train_loader, val_loader, device, epochs_n=1, plot_each=1):
    # send the model to that device
    model = model.to(device)
    # initialize some visualization code that you don't need to care about
    monitor = seminar_utils.Monitor()
    # one full cycle on train data is called epoch
    for epoch in trange(epochs_n):
        # switch the model to the train mode
        # later on we will have some modules that function differently in train and test mode
        model.train()
        for model_input, target in train_loader:
            # send data to device
            model_input = model_input.to(device)
            #print(model_input)
            target = target.to(device)
            # calculate outputs and loss
            model_output = model(model_input)
            loss = loss_function(model_output, target)
            #print("Loss_function = ",loss, "DATA = ", model_output, target)
            # update model weights
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            # do some visualization
            monitor.add_loss_value(loss.item())
        
        # evaluate the model
        # switch the model to the test mode
        model.eval()
        train_accuracy = seminar_utils.get_accuracy(model, train_loader, device)
        val_accuracy = seminar_utils.get_accuracy(model, val_loader, device)
        monitor.add_train_accuracy_value(train_accuracy)
        monitor.add_val_accuracy_value(val_accuracy)
        
        if epoch % plot_each == 0:
            monitor.show()

    #plt.savefig("Result:\n"+
    #        "b_s="+str(batch_size)+"\n"+
    #        "h_d_sp="+str(hidden_dim_size)+"\n"+
    #        "B_4_no_trend="+str(Boundary_for_neutral_trend)+"%\n"+
    #        "loss_function="+str(loss_function)+"\n"+
    #        "epochs="+str(epochs) +
    #        ".png", dpi = 400, quality = 100)

"""Задаём необходимые архитектурные параметры и обучаем нейронную сеть"""

seminar_utils.set_random_seeds(device=device)
#batch_size = inputs.shape[1]
batch_size = 32
epochs = 50
hidden_dim_size = 32
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4)

model = MLP(input_size = inputs.shape[1], hidden_dim_size =  hidden_dim_size, number_of_targets = 3)
#loss_function = torch.nn.MSELoss()
loss_function =torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)
#optimizer = torch.optim.Adam(model.parameters())
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=(int(epochs/10), int(epochs/3)), gamma=1/10)

train(model, optimizer, loss_function, train_loader, val_loader, device, epochs_n=epochs)
#plt.savefig("Result:\n"+
#            "b_s="+str(batch_size)+"\n"+
#            "h_d_sp="+str(hidden_dim_size)+"\n"+
#            "B_4_no_trend="+str(Boundary_for_neutral_trend)+"%\n"+
#            "loss_function="+str(loss_function)+"\n"+
#            "epochs="+str(epochs) +
#            ".png", dpi = 400, quality = 100)

"""Сеть поглубже"""

class MLP_2(torch.nn.Module):
    def __init__(self, input_size, hidden_dim_size, number_of_targets):
        r"""
        Parameters
        ----------
        input_size : int
        hidden_dim_size : int
        number_of_targets : int
        """
        super().__init__()
        
        #input_size = input_image_size[0] * input_image_size[1] * input_image_size[2]
        #number_of_targets = 1
        self.mlp = torch.nn.Sequential(
            torch.nn.Linear(input_size, hidden_dim_size),
            torch.nn.ReLU(),
            torch.nn.BatchNorm1d(hidden_dim_size),
            torch.nn.Linear(hidden_dim_size, hidden_dim_size),
            torch.nn.ReLU(),
            torch.nn.BatchNorm1d(hidden_dim_size),
            torch.nn.Linear(hidden_dim_size, hidden_dim_size),
            torch.nn.ReLU(),
            torch.nn.BatchNorm1d(hidden_dim_size),
            torch.nn.Linear(hidden_dim_size, number_of_targets)
        )
        
    def forward(self, x):
        r"""
        Parameters
        ----------
        x : torch.Tensor
            of shape [batch_size, vector_size]
        
        Returns
        -------
        y : torch.Tensor
            of shape [batch_size, 1]
        """
        batch_size = len(x)
        x = x.reshape(batch_size, -1)
        return self.mlp(x)

"""В конце данной тренировки смотрим на распределение по классам полученных ответов на $(epochs_n - 2)$ эпохе"""

# we choose the device that we will work on --- GPU or CPU
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')


def train(model, optimizer, loss_function, train_loader, val_loader, device, epochs_n=1, plot_each=1):
    # send the model to that device
    model = model.to(device)
    flag = 0
    # initialize some visualization code that you don't need to care about
    monitor = seminar_utils.Monitor()
    # one full cycle on train data is called epoch
    for epoch in trange(epochs_n):
        # switch the model to the train mode
        # later on we will have some modules that function differently in train and test mode
        model.train()
        for model_input, target in train_loader:
            # send data to device
            model_input = model_input.to(device)
            #print(model_input)
            target = target.to(device)
            # calculate outputs and loss
            model_output = model(model_input)
            loss = loss_function(model_output, target)
            #print("Loss_function = ",loss, "DATA = ", model_output, target)
            # update model weights
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            # do some visualization
            monitor.add_loss_value(loss.item())
        
        # evaluate the model
        # switch the model to the test mode
        model.eval()
        train_accuracy = seminar_utils.get_accuracy(model, train_loader, device)
        val_accuracy = seminar_utils.get_accuracy(model, val_loader, device)
        monitor.add_train_accuracy_value(train_accuracy)
        monitor.add_val_accuracy_value(val_accuracy)
        
        if epoch % plot_each == 0:
            monitor.show()

        if (epoch == epochs_n - 2):
          save_list_pred = []
          with torch.no_grad():
            for x, y in val_loader:
              x = x.to(device)
              y = y.to(device)
              prediction = model(x).argmax(dim=-1, keepdim=True)
              save_list_pred.append(prediction)
        if (epoch == epochs_n - 1):
          #print(save_list_pred)
          general_answer = []
          for tens in save_list_pred:
            for elem in tens:
              general_answer.append(elem.tolist()[0])
          print(general_answer.count(0), general_answer.count(1), general_answer.count(2))
          vals = [general_answer.count(0), general_answer.count(1), general_answer.count(2)]
          labels = ["Убывающий", "боковой", "Восходящий"]
          fig, ax = plt.subplots()
          ax.pie(vals, labels=labels)
          ax.axis("equal")

seminar_utils.set_random_seeds(device=device)
#batch_size = inputs.shape[1]
batch_size = 32
epochs = 50
hidden_dim_size = 32
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4)

model = MLP_2(input_size = inputs.shape[1], hidden_dim_size =  hidden_dim_size, number_of_targets = 3)
#loss_function = torch.nn.MSELoss()
loss_function =torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)
#optimizer = torch.optim.Adam(model.parameters())
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=(int(epochs/10), int(epochs/3)), gamma=1/10)

train(model, optimizer, loss_function, train_loader, val_loader, device, epochs_n=epochs)

"""пара LSTM клеток внутри линейных слоёв"""

from torch.nn.modules.rnn import LSTMCell
class LSTM(torch.nn.Module):
    def __init__(self, input_size, hidden_dim_size, number_of_targets):
        r"""
        Parameters
        ----------
        input_size : int
        hidden_dim_size : int
        number_of_targets : int
        """
        super(LSTM, self).__init__()

        self.hidden_layers = 64
        self.hidden_layers2 = 32
        
        #input_size = input_image_size[0] * input_image_size[1] * input_image_size[2]
        #number_of_targets = 1

        self.stage1 = torch.nn.Sequential(
            torch.nn.Linear(input_size, hidden_dim_size),
            #torch.nn.RNN(input_size, 20, hidden_dim_size),
            torch.nn.ReLU(),
            torch.nn.BatchNorm1d(hidden_dim_size),  ## <---
            torch.nn.Linear(hidden_dim_size, 3)
        )
        self.lstm1 = torch.nn.LSTMCell(3, self.hidden_layers)
        self.lstm2 = torch.nn.LSTMCell(self.hidden_layers, self.hidden_layers2)
        self.stage2 = torch.nn.Sequential(
            torch.nn.Linear(self.hidden_layers2, hidden_dim_size),
            torch.nn.ReLU(),
            torch.nn.BatchNorm1d(hidden_dim_size),
            torch.nn.Linear(hidden_dim_size, number_of_targets)
        )
        
    def forward(self, x):
        r"""
        Parameters
        ----------
        x : torch.Tensor
            of shape [batch_size, vector_size]
        
        Returns
        -------
        y : torch.Tensor
            of shape [batch_size, 1]
        """
        batch_size = len(x)
        x = x.reshape(batch_size, -1)
        #print(x)
        x = self.stage1(x)
        #print(x.shape)
        h_1, c_1 = self.lstm1(x)
        h_2, c_2 = self.lstm2(h_1)
        return self.stage2(h_2)

seminar_utils.set_random_seeds(device=device)
#batch_size = inputs.shape[1]
batch_size = 32
epochs = 500
hidden_dim_size = 120
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4)

model = LSTM(input_size = inputs.shape[1], hidden_dim_size =  hidden_dim_size, number_of_targets = 3)
#loss_function = torch.nn.MSELoss()
loss_function =torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)
#optimizer = torch.optim.Adam(model.parameters())
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=(int(epochs/10), int(epochs/3)), gamma=1/10)

train(model, optimizer, loss_function, train_loader, val_loader, device, epochs_n=epochs)

"""Пара RNN клеток внутри линейных слоёв"""

from torch.nn.modules.rnn import RNNCell
class RNN(torch.nn.Module):
    def __init__(self, input_size, hidden_dim_size, number_of_targets):
        r"""
        Parameters
        ----------
        input_size : int
        hidden_dim_size : int
        number_of_targets : int
        """
        super(RNN, self).__init__()

        self.hidden_size = 128
        self.hidden_size2 = 32
        
        #input_size = input_image_size[0] * input_image_size[1] * input_image_size[2]
        #number_of_targets = 1

        self.stage1 = torch.nn.Sequential(
            torch.nn.Linear(input_size, hidden_dim_size),
            #torch.nn.RNN(input_size, 20, hidden_dim_size),
            torch.nn.ReLU(),
            torch.nn.BatchNorm1d(hidden_dim_size),  ## <---
            torch.nn.Linear(hidden_dim_size, 120)
        )
        self.rnn1 = torch.nn.RNNCell(120, self.hidden_size)
        self.rnn2 = torch.nn.RNNCell(self.hidden_size, self.hidden_size2)
        self.stage2 = torch.nn.Sequential(
            torch.nn.Linear(self.hidden_size2, hidden_dim_size),
            torch.nn.LeakyReLU(),
            torch.nn.BatchNorm1d(hidden_dim_size),
            torch.nn.Linear(hidden_dim_size, number_of_targets)
        )
        
    def forward(self, x):
        r"""
        Parameters
        ----------
        x : torch.Tensor
            of shape [batch_size, vector_size]
        
        Returns
        -------
        y : torch.Tensor
            of shape [batch_size, 1]
        """
        batch_size = len(x)
        x = x.reshape(batch_size, -1)
        #print(x)
        x = self.stage1(x)
        #print(x.shape)
        h_1 = self.rnn1(x)
        h_2 = self.rnn2(h_1)
        return self.stage2(h_2)

seminar_utils.set_random_seeds(device=device)
#batch_size = inputs.shape[1]
batch_size = 32
epochs = 15
hidden_dim_size = 120
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4)

model = RNN(input_size = inputs.shape[1], hidden_dim_size =  hidden_dim_size, number_of_targets = 3)
#loss_function = torch.nn.MSELoss()
loss_function =torch.nn.CrossEntropyLoss()
#optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)
#optimizer = torch.optim.Adam(model.parameters())
#optimizer = torch.optim.Adagrad(model.parameters())
optimizer = torch.optim.RMSprop(model.parameters())
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=(int(epochs/10), int(epochs/3)), gamma=1/10)

train(model, optimizer, loss_function, train_loader, val_loader, device, epochs_n=epochs)

"""Убедимся, что параметр границы тренда определён разумно"""

print(Key_ticker_values_list_test.count(0), Key_ticker_values_list_test.count(1), Key_ticker_values_list_test.count(2))
vals = [Key_ticker_values_list_test.count(0), Key_ticker_values_list_test.count(1), Key_ticker_values_list_test.count(2)]
labels = ["Убывающий", "боковой", "Восходящий"]
fig, ax = plt.subplots()
ax.pie(vals, labels=labels)
ax.axis("equal")

"""Дальнейший код не является допустимым к запуску... Это лишь наброски."""

class CNNBN(torch.nn.Module):
    def __init__(self, input_size, number_of_targets):
        super().__init__()
        in_channels = input_size
        
        self.stage1 = torch.nn.Sequential(
            torch.nn.Conv1d(in_channels=in_channels, out_channels=8, kernel_size=3, padding=1),
            torch.nn.BatchNorm1d(8),  ## <---
            torch.nn.Conv1d(in_channels=8, out_channels=8, kernel_size=3, padding=1),
            torch.nn.ReLU(),
            torch.nn.MaxPool1d(kernel_size=2),
            torch.nn.BatchNorm1d(8),  ## <---
        )
        self.stage2 = torch.nn.Sequential(
            torch.nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, padding=1),
            torch.nn.BatchNorm1d(16),  ## <---
            torch.nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1),
            torch.nn.ReLU(),
            torch.nn.MaxPool1d(kernel_size=2),
            torch.nn.BatchNorm1d(16),  ## <---
        )
        self.fc = torch.nn.Sequential(
            torch.nn.Linear(16, 20),
            torch.nn.ReLU(),
            torch.nn.BatchNorm1d(20),  ## <---
            torch.nn.Linear(20, number_of_targets)
        )
        
    def forward(self, x):
        batch_size = x.shape[0]
        
        x = self.stage1(x)
        x = self.stage2(x)
        x = x.reshape(batch_size, -1)
        x = self.fc(x)
        return x

class MLP_for_classification(torch.nn.Module):
    def __init__(self, input_size, hidden_dim_size, number_of_targets):
        r"""
        Parameters
        ----------
        input_size : int
        hidden_dim_size : int
        number_of_targets : int
        """
        super().__init__()
        print(input_size, number_of_targets)
        #input_size = input_image_size[0] * input_image_size[1] * input_image_size[2]
        self.mlp = torch.nn.Sequential(
            #torch.nn.Linear(input_size, hidden_dim_size),
            #torch.nn.ReLU(),
            #torch.nn.Linear(hidden_dim_size, number_of_targets)
            torch.nn.Linear(input_size, number_of_targets)
        )
        
    def forward(self, x):
        r"""
        Parameters
        ----------
        x : torch.Tensor
            of shape [batch_size, vector_size]
        
        Returns
        -------
        y : torch.Tensor
            of shape [batch_size, 1]
        """
        batch_size = 64
        print("THIS IS XXX = ",x)
        #x = x.reshape(batch_size, -1)
        return self.mlp(x)

batch_size = len(train_dataset[0][0])
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4)

inputs, targets = next(iter(train_loader))
#inputs.shape
model = MLP_for_classification(input_size = inputs.shape[1], hidden_dim_size = 120, number_of_targets = 3)
nn_outputs = model(inputs)
nn_outputs.shape
print(nn_outputs)

# we choose the device that we will work on --- GPU or CPU
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
#device = torch.device('cpu')

def train_for_Classification(model, optimizer, loss_function, train_loader, val_loader, device, epochs_n=1, plot_each=1):
    # send the model to that device
    model = model.to(device)
    # initialize some visualization code that you don't need to care about
    monitor = seminar_utils.Monitor()
    # one full cycle on train data is called epoch
    for epoch in trange(epochs_n):
        # switch the model to the train mode
        # later on we will have some modules that function differently in train and test mode
        model.train()
        for model_input, target in train_loader:
            #print(model_input, "and", target)
            # send data to device
            model_input = model_input.to(device)
            target = target.to(device)
            # calculate outputs and loss
            print("INPUT = ", model_input)
            print("Target = ", target)
            model_output = model(model_input)
            print("OUTPUT = ", model_output)
            loss = loss_function(model_output, target)
            # update model weights
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            # do some visualization
            monitor.add_loss_value(loss.item())
        
        # evaluate the model
        # switch the model to the test mode
        model.eval()
        train_accuracy = seminar_utils.get_accuracy(model, train_loader, device)
        val_accuracy = seminar_utils.get_accuracy(model, val_loader, device)
        print(train_accuracy, "with", val_accuracy)
        monitor.add_train_accuracy_value(train_accuracy)
        monitor.add_val_accuracy_value(val_accuracy)
        
        if epoch % plot_each == 0:
            monitor.show()

seminar_utils.set_random_seeds(device=device)
#batch_size = len(train_dataset[0][0])
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4)

model = MLP_for_classification(input_size = len(train_dataset[0][0]), hidden_dim_size = 120, number_of_targets = 3)
loss_function = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)

train_for_Classification(model, optimizer, loss_function, train_loader, val_loader, device, epochs_n=10, plot_each = 1)
'''